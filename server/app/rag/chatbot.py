"""ì²œì•ˆ ë„ì‹œì¬ìƒì§€ì›ì„¼í„° ì±—ë´‡ â€“ ë¡œì»¬ ë¬¸ì„œ ìš°ì„ (data ë””ë ‰í† ë¦¬) â†’ í¼ì§€ ë§¤ì¹­ â†’ ì›¹ ê²€ìƒ‰ â†’ LLM ê¸°ë³¸ ì§€ì‹"""

from __future__ import annotations

import os
import textwrap
from typing import Tuple, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools import DuckDuckGoSearchRun

from rapidfuzz import fuzz, process            # í¼ì§€ ê²€ìƒ‰ í™œìš©

from .prompt import PROMPT
from .retriever import get_retriever
from .reranker import rerank
from .verifier import fact_check

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_LLM = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
    temperature=0.2,
    top_p=0.9,
)
_SYS = SystemMessage(
    content=(
        "ë„ˆëŠ” ì²œì•ˆ ë„ì‹œì¬ìƒì§€ì›ì„¼í„° ì „ìš© ì±—ë´‡ì´ì•¼. "
        "ì •í™•í•˜ì§€ ì•Šì€ ì§ˆë¬¸ì´ë¼ë„ ìµœëŒ€í•œ ìœ ì¶”í•´ì„œ ë‹µë³€í•˜ê³ , "
        "í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” 'ì œê°€ ì•Œê³  ìˆëŠ” ë²”ìœ„ì—ì„œ ë§ì”€ë“œë¦´ê²Œìš”'ì²˜ëŸ¼ ë¶€ë“œëŸ½ê²Œ í‘œí˜„í•´. "
        "ì ˆëŒ€ë¡œ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë˜ëŠ” 'ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤' ê°™ì€ ë§ë¡œ ëë‚´ì§€ ë§ˆ."
    )
)














# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
THRESH      = float(os.getenv("THRESH",      0.15))   # ë¡œì»¬ ì¬ë­í¬ ì„ê³„
TOP_K       = int(os.getenv("TOP_K",         4))      # ì¬ë­í¬ í›„ ì‚¬ìš©í•  ë¬¸ì„œ ìˆ˜
FUZZ_LIMIT  = int(os.getenv("FUZZ_LIMIT",    3))      # í¼ì§€ í›„ë³´ ê°œìˆ˜
FUZZ_SCORE  = int(os.getenv("FUZZ_SCORE",    70))     # í¼ì§€ ìµœì†Œ ì ìˆ˜(0~100)
SEARCH_HITS = int(os.getenv("SEARCH_HITS",   5))      # DuckDuckGo ê²°ê³¼ ìˆ˜

_SEARCH = DuckDuckGoSearchRun(backend="auto")

def _normalize_q(q: str) -> str:
    """ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©"""
    return q

def _local_ctx(q: str) -> Tuple[str, float]:
    raw = [d.page_content for d in get_retriever().get_relevant_documents(q)]
    docs, best = rerank(q, raw, top_n=TOP_K) if raw else ([], 0.0)
    ctx = "\n\n".join(textwrap.shorten(d, 400) for d in docs)
    return ctx, best

def _fuzzy_ctx(q: str) -> str:
    """í¸ì§‘ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ì¶”ì¶œ (RapidFuzz)"""
    vect_ret = get_retriever().retrievers[0]
    docs = vect_ret.vectorstore.docstore._dict.values()
    texts = [d.page_content for d in docs]
    pairs = process.extract(q, texts, scorer=fuzz.partial_ratio, limit=FUZZ_LIMIT)
    good = [item for item, score, _ in pairs if score >= FUZZ_SCORE]
    return "\n\n".join(textwrap.shorten(t, 400) for t in good)

def _web_ctx(q: str) -> str:
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜ì—´í˜• í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    try:
        hits = _SEARCH.results(q, num_results=SEARCH_HITS)
    except Exception:
        return ""
    lines: list[str] = []
    for h in hits:
        txt = h.get("body") or h.get("snippet") or h.get("title", "")
        url = h.get("href") or h.get("url", "")
        lines.append(f"- {txt} (ì¶œì²˜: {url})")
    return "\n".join(lines)


def _ask_llm(question: str, ctx: str | None) -> str:
    prompt = PROMPT.format(
        context=ctx or "í•´ë‹¹ ë‚´ìš©ì€ ë¬¸ì„œì— ì—†ìŒ",
        question=question,
    )
    return _LLM.invoke([_SYS, HumanMessage(content=prompt)]).content.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë£° ê¸°ë°˜ ì¦‰ë‹µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rule_based_answer(user_input: str) -> str | None:
    q = user_input.lower()

    # â–¶ ì‹ ì²­ ìê²© ì§ˆë¬¸ (â†’ ë‚˜ë„ ê°€ëŠ¥í•´?)
    if any(kw in q for kw in ["ì‹ ì²­í•  ìˆ˜", "ì°¸ì—¬í•  ìˆ˜", "ë ê¹Œìš”", "ì‹ ì²­ ê°€ëŠ¥í• ê¹Œìš”", "ë‚˜", "ì €"]) and "íˆ¬ì–´" not in q:
        return (
            "ë„ì‹œì¬ìƒ í”„ë¡œê·¸ë¨ì€ ì²œì•ˆì‹œì— ê±°ì£¼í•˜ê±°ë‚˜ ê´€ì‹¬ ìˆëŠ” ì‹œë¯¼ ëˆ„êµ¬ë‚˜ ì‹ ì²­í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”. "
            "í”„ë¡œê·¸ë¨ì— ë”°ë¼ ì¡°ê±´ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ê¼­ í™•ì¸í•´ì£¼ì„¸ìš”!"
        )

    # â–¶ í˜„ì¬ í”„ë¡œê·¸ë¨ ì§ˆë¬¸
    if any(kw in q for kw in ["í˜„ì¬", "ì§„í–‰", "í”„ë¡œê·¸ë¨", "ì„¸ë¯¸ë‚˜", "ì§€ê¸ˆ í•˜ê³ ", "ì°¸ì—¬ ê°€ëŠ¥í•œ", "ì‹ ì²­ ê°€ëŠ¥í•œ"]) and "íˆ¬ì–´" not in q:
        return (
            "í˜„ì¬ ì§„í–‰ ì¤‘ì¸ í”„ë¡œê·¸ë¨ì€ 'ë¡œì»¬ë¸Œëœë”© í™œìš©ê³¼ í™•ì‚°ì „ëµ ì •ì±… ì„¸ë¯¸ë‚˜'ê°€ ìˆìŠµë‹ˆë‹¤."
        )

    # â–¶ íˆ¬ì–´ ì‹ ì²­ ìê²© ì§ˆë¬¸
    if "íˆ¬ì–´" in q and any(kw in q for kw in ["ì‹ ì²­", "ìê²©", "ëˆ„ê°€", "ê°€ëŠ¥", "í•  ìˆ˜"]):
        return (
            "ë„ì‹œì¬ìƒ íˆ¬ì–´ëŠ” ì²œì•ˆì‹œ ë„ì‹œì¬ìƒì‚¬ì—…ì— ê´€ì‹¬ ìˆëŠ” ì‹œë¯¼ ëˆ„êµ¬ë‚˜ ì‹ ì²­í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”. "
            "ë˜í•œ ì²œì•ˆì‹œ ì„ ì§„ì§€ ë‹µì‚¬ë¥¼ í¬ë§í•˜ëŠ” ì™¸ë¶€ ì„¼í„°ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )

    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask(question: str) -> str:
    # ğŸ”¹ (0) ë£° ê¸°ë°˜ ìš°ì„  ì‘ë‹µ
    rule_answer = rule_based_answer(question)
    if rule_answer:
        return f"{rule_answer}\n\nâ–²confidence: Exact (rule-based)"

    # 0) ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    norm_q = _normalize_q(question)
    print(f"[DEBUG] raw='{question}' â†’ norm='{norm_q}'")

    # 1) ë¡œì»¬ ë²¡í„°Â·BM25 ê²€ìƒ‰ + Cross-Encoder ì¬ë­í¬
    ctx, best = _local_ctx(norm_q)
    print(f"[DEBUG] local best={best:.3f}")

    # 1-a) ë¡œì»¬ ì»¨í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ì‹ ë¢°ë˜ë©´ ë°”ë¡œ ì‘ë‹µ
    if ctx and best >= THRESH:
        ans = _ask_llm(norm_q, ctx)
        return f"{ans}\n\nâ–²confidence: High (local)"

    # 2) í¼ì§€ ë§¤ì¹­ ë³´ê°• (ë¡œì»¬ ìœ ì‚¬ì„±)
    fuzzy = _fuzzy_ctx(norm_q)
    if fuzzy:
        ans = _ask_llm(norm_q, fuzzy)
        if fact_check(norm_q, ans):
            return f"{ans}\n\nâ–²confidence: Mid (fuzzy)"

    # 3) ì›¹ ê²€ìƒ‰ ë³´ê°• (ë¡œì»¬Â·í¼ì§€ ëª¨ë‘ ì—†ì„ ë•Œ)
    web_ctx = _web_ctx(norm_q)
    if web_ctx:
        # ì›¹ ê²°ê³¼ ì „ì²´ë¥¼ ì´ìš©í•´ ìƒì„¸í•˜ê²Œ ë‹µë³€
        ans = _ask_llm(norm_q, web_ctx)
        return f"{ans}\n\nâ–²confidence: Mid (web)"

    # 4) LLM ìì²´ ì§€ì‹ (ëª¨ë‘ ì‹¤íŒ¨ ì‹œ)
    ans = _ask_llm(norm_q, "")
    return f"{ans}\n\nâ–²confidence: Low (model-only)"
