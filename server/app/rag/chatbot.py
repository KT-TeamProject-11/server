"""ì²œì•ˆ ë„ì‹œì¬ìƒì§€ì›ì„¼í„° ì±—ë´‡ â€“ ë¡œì»¬ ë¬¸ì„œ ìš°ì„ (data ë””ë ‰í† ë¦¬) â†’ í¼ì§€ ë§¤ì¹­ â†’ ì›¹ ê²€ìƒ‰ â†’ LLM ê¸°ë³¸ ì§€ì‹"""

from __future__ import annotations

import os
import textwrap
from typing import Tuple, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools import DuckDuckGoSearchRun

from rapidfuzz import fuzz, process            # í¼ì§€ ê²€ìƒ‰ í™œìš©

from .prompt import PROMPT
from .retriever import get_retriever
from .reranker import rerank
from .verifier import fact_check

from .programs import get_program_url
from .utils import extract_program_name

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_LLM = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
    temperature=0.2,
    top_p=0.9,
)
_SYS = SystemMessage(
    content=(
        "ë„ˆëŠ” ì²œì•ˆ ë„ì‹œì¬ìƒì§€ì›ì„¼í„° ì „ìš© ì±—ë´‡ì´ì•¼. "
        "ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ìµœëŒ€í•œ ì°¾ì•„ë³´ê³ , ë§ˆì§€ë§‰ê¹Œì§€ ë‹µì„ ë§Œë“¤ì–´ ì¤˜. "
        "ì ˆëŒ€ë¡œ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ê°™ì€ ë§ë¡œ ëë‚´ì§€ ë§ˆ."
    )
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
THRESH      = float(os.getenv("THRESH",      0.15))   # ë¡œì»¬ ì¬ë­í¬ ì„ê³„
TOP_K       = int(os.getenv("TOP_K",         4))      # ì¬ë­í¬ í›„ ì‚¬ìš©í•  ë¬¸ì„œ ìˆ˜
FUZZ_LIMIT  = int(os.getenv("FUZZ_LIMIT",    3))      # í¼ì§€ í›„ë³´ ê°œìˆ˜
FUZZ_SCORE  = int(os.getenv("FUZZ_SCORE",    70))     # í¼ì§€ ìµœì†Œ ì ìˆ˜(0~100)
SEARCH_HITS = int(os.getenv("SEARCH_HITS",   5))      # DuckDuckGo ê²°ê³¼ ìˆ˜

_SEARCH = DuckDuckGoSearchRun(backend="auto")

def _normalize_q(q: str) -> str:
    """ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©"""
    return q

def _local_ctx(q: str) -> Tuple[str, float]:
    raw = [d.page_content for d in get_retriever().get_relevant_documents(q)]
    docs, best = rerank(q, raw, top_n=TOP_K) if raw else ([], 0.0)
    ctx = "\n\n".join(textwrap.shorten(d, 400) for d in docs)
    return ctx, best

def _fuzzy_ctx(q: str) -> str:
    """í¸ì§‘ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ì¶”ì¶œ (RapidFuzz)"""
    vect_ret = get_retriever().retrievers[0]
    docs = vect_ret.vectorstore.docstore._dict.values()
    texts = [d.page_content for d in docs]
    pairs = process.extract(q, texts, scorer=fuzz.partial_ratio, limit=FUZZ_LIMIT)
    good = [item for item, score, _ in pairs if score >= FUZZ_SCORE]
    return "\n\n".join(textwrap.shorten(t, 400) for t in good)

def _web_ctx(q: str) -> str:
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜ì—´í˜• í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    try:
        hits = _SEARCH.results(q, num_results=SEARCH_HITS)
    except Exception:
        return ""
    lines: list[str] = []
    for h in hits:
        txt = h.get("body") or h.get("snippet") or h.get("title", "")
        url = h.get("href") or h.get("url", "")
        lines.append(f"- {txt} (ì¶œì²˜: {url})")
    return "\n".join(lines)


def _ask_llm(question: str, ctx: str | None) -> str:
    prompt = PROMPT.format(
        context=ctx or "í•´ë‹¹ ë‚´ìš©ì€ ë¬¸ì„œì— ì—†ìŒ",
        question=question,
    )
    return _LLM.invoke([_SYS, HumanMessage(content=prompt)]).content.strip()

def _program_link_ctx(q: str) -> Tuple[str | None, str | None]:
    prog_name = extract_program_name(q)
    if not prog_name:
        return None, None
    url = get_program_url(prog_name)
    return prog_name, url

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask(question: str) -> str:
    # 0) ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    norm_q = _normalize_q(question)
    print(f"[DEBUG] raw='{question}' â†’ norm='{norm_q}'")

    # 0-a) í”„ë¡œê·¸ë¨ ë§í¬ ì „ìš© ì§ˆë¬¸ í•„í„°ë§
    prog, url = _program_link_ctx(norm_q)
    if prog:
        if url:
            # (ë§í¬ë§Œ) f"'{prog}'ì€(ëŠ”) ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\nğŸ‘‰ {url}\n\nâ–²confidence: Rule (program)"
            return (f"'{prog}' ì •ë³´ëŠ” ì•„ë˜ ë§í¬ì—ì„œ ë°”ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n"
                            f"[**{prog} í™ˆí˜ì´ì§€ ë°”ë¡œê°€ê¸°**]({url})\n\n"
                            f"â–²confidence: Rule (program)")        
        else:
            return f"'{prog}'ì— ëŒ€í•œ ê³µì‹ ì‚¬ì´íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nâ–²confidence: Rule (not found)"

    # 1) ë¡œì»¬ ë²¡í„°Â·BM25 ê²€ìƒ‰ + Cross-Encoder ì¬ë­í¬
    ctx, best = _local_ctx(norm_q)
    print(f"[DEBUG] local best={best:.3f}")

    # 1-a) ë¡œì»¬ ì»¨í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ì‹ ë¢°ë˜ë©´ ë°”ë¡œ ì‘ë‹µ
    if ctx and best >= THRESH:
        ans = _ask_llm(norm_q, ctx)
        return f"{ans}\n\nâ–²confidence: High (local)"

    # 2) í¼ì§€ ë§¤ì¹­ ë³´ê°• (ë¡œì»¬ ìœ ì‚¬ì„±)
    fuzzy = _fuzzy_ctx(norm_q)
    if fuzzy:
        ans = _ask_llm(norm_q, fuzzy)
        if fact_check(norm_q, ans):
            return f"{ans}\n\nâ–²confidence: Mid (fuzzy)"

    # 3) ì›¹ ê²€ìƒ‰ ë³´ê°• (ë¡œì»¬Â·í¼ì§€ ëª¨ë‘ ì—†ì„ ë•Œ)
    web_ctx = _web_ctx(norm_q)
    if web_ctx:
        # ì›¹ ê²°ê³¼ ì „ì²´ë¥¼ ì´ìš©í•´ ìƒì„¸í•˜ê²Œ ë‹µë³€
        ans = _ask_llm(norm_q, web_ctx)
        return f"{ans}\n\nâ–²confidence: Mid (web)"

    # 4) LLM ìì²´ ì§€ì‹ (ëª¨ë‘ ì‹¤íŒ¨ ì‹œ)
    ans = _ask_llm(norm_q, "")
    return f"{ans}\n\nâ–²confidence: Low (model-only)"
