"""ì²œì•ˆ ë„ì‹œì¬ìƒì§€ì›ì„¼í„° ì±—ë´‡ â€“ RAG + ë£° + í¼ì§€ + ì›¹ + ì¢…í•© ì»¨í…ìŠ¤íŠ¸ ê²Œì´íŒ… + Redis ìºì‹œ"""

from __future__ import annotations

import os
import hashlib
import textwrap
import asyncio
import contextlib
import re # ğŸ”´ì¶”ê°€
from typing import Tuple, Optional

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.tools import DuckDuckGoSearchRun
from rapidfuzz import fuzz, process
from redis.asyncio import Redis

from .prompt import PROMPT, ALL_SOURCES_PROMPT
from .retriever import get_retriever
from .reranker import rerank
from .verifier import fact_check
from .programs import get_all_aliases, get_all_tags, get_program_by_alias, get_programs_by_tag
from utils.intent_classifier import classify_intent_and_extract_entity

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_MODEL   = os.environ["OPENAI_MODEL"]
LLAMA_API_URL  = os.environ["LLAMA_API"]
REDIS_URL      = os.environ["REDIS_URL"]
CACHE_TTL      = int(os.environ["CACHE_TTL_SEC"])
THRESH         = float(os.environ["THRESH"])
TOP_K          = int(os.environ["TOP_K"])
FUZZ_LIMIT     = int(os.environ["FUZZ_LIMIT"])
FUZZ_SCORE     = int(os.environ["FUZZ_SCORE"])
SEARCH_HITS    = int(os.environ["SEARCH_HITS"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Redis ìºì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_redis = Redis.from_url(REDIS_URL, encoding="utf-8", decode_responses=True)

async def _get_cached(key: str) -> Optional[str]:
    with contextlib.suppress(Exception):
        return await _redis.get(key)
    return None

async def _set_cached(key: str, val: str):
    with contextlib.suppress(Exception):
        await _redis.set(key, val, ex=CACHE_TTL)

def _cache_key(q: str, ctx: str) -> str:
    digest = hashlib.sha256(f"{q}::{ctx}".encode()).hexdigest()
    return f"llm_cache:{digest}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM ì¸ìŠ¤í„´ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SYS = SystemMessage(
    content="ë„ˆëŠ” ì²œì•ˆ ë„ì‹œì¬ìƒì§€ì›ì„¼í„° ì „ìš© ì±—ë´‡ì´ì•¼. í•„ìš”í•œ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì¤˜."
)
_LLM        = ChatOpenAI(model=OPENAI_MODEL, temperature=0.2, top_p=0.9)
_LLM_LOCAL  = ChatOpenAI(base_url=LLAMA_API_URL, api_key="none",
                         model="llama-3-8b-instruct-q4", temperature=0.2)
_LLM_BACKUP = _LLM
_SEARCH     = DuckDuckGoSearchRun(backend="auto")

def _normalize(q: str) -> str:
    return q.strip()

# ğŸ”´ì¶”ê°€
def _linkify(text: str) -> str:
    """ì‘ë‹µ ë‚´ URLì„ í•˜ì´í¼ë§í¬ë¡œ ë³€í™˜"""
    url_pattern = re.compile(r'(https?://[^\s]+)')
    return url_pattern.sub(r'<a href="\1" target="_blank">\1</a>', text)

def _local_ctx(q: str) -> Tuple[str, float]:
    docs = [d.page_content for d in get_retriever().get_relevant_documents(q)]
    top_docs, best = rerank(q, docs, top_n=TOP_K) if docs else ([], 0.0)
    ctx = "\n\n".join(textwrap.shorten(d, 400) for d in top_docs)
    return ctx, best



# def _local_ctx(q: str) -> Tuple[str, float]:
#     raw_docs = get_retriever().get_relevant_documents(q)

#     # ğŸŸ¡ ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
#     print("\nğŸŸ¡ [DEBUG] Retrieved Documents:")
#     for i, d in enumerate(raw_docs):
#         print(f"  {i+1}.")
#         print(f"    > Metadata: {d.metadata}")
#         print(f"    > Page content (ì•ë¶€ë¶„): {d.page_content[:150]}")

#     docs = [d.page_content for d in raw_docs]
#     top_docs, best = rerank(q, docs, top_n=TOP_K) if docs else ([], 0.0)
#     ctx = "\n\n".join(textwrap.shorten(d, 400) for d in top_docs)
#     return ctx, best




def _fuzzy_ctx(q: str) -> str:
    vect = get_retriever().retrievers[0]
    texts = [d.page_content for d in vect.vectorstore.docstore._dict.values()]
    pairs = process.extract(q, texts, scorer=fuzz.partial_ratio, limit=FUZZ_LIMIT)
    chosen = [t for t, score, _ in pairs if score >= FUZZ_SCORE]
    return "\n\n".join(textwrap.shorten(t, 400) for t in chosen)

def _web_ctx(q: str) -> str:
    with contextlib.suppress(Exception):
        hits = _SEARCH.results(q, num_results=SEARCH_HITS)
        return "\n".join(
            f"- {h.get('body') or h.get('title','')} (ì¶œì²˜: {h.get('url','')})"
            for h in hits
        )
    return ""

def _llm_call(llm: ChatOpenAI, q: str, ctx: str) -> str:
    prompt = PROMPT.format(context=ctx or "ì—†ìŒ", question=q)
    return llm.invoke([_SYS, HumanMessage(content=prompt)]).content.strip()

async def _gate_llm(q: str, ctx: str) -> Tuple[str, str]:
    # 1) ë¡œì»¬ llama ì‹œë„
    with contextlib.suppress(Exception):
        ans = _llm_call(_LLM_LOCAL, q, ctx)
        if fact_check(q, ans):
            return ans, "Local"
    # 2) OpenAI ë°±ì—…
    ans = _llm_call(_LLM_BACKUP, q, ctx)
    return ans, "Backup"

def _ask_all_sources(q: str, local_ctx: str, rule_ctx: str, web_ctx: str) -> str:
    prompt = ALL_SOURCES_PROMPT.format(
        local_ctx=local_ctx or "ì—†ìŒ",
        rule_ctx=rule_ctx or "ì—†ìŒ",
        web_ctx=web_ctx or "ì—†ìŒ",
        question=q,
    )
    return _LLM.invoke([_SYS, HumanMessage(content=prompt)]).content.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ë¹„ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def ask_async(question: str) -> str:
    q = _normalize(question)
    blank_key = _cache_key(q, "")

    # 0) ìºì‹œ ìš°ì„ 
    if (cached := await _get_cached(blank_key)):
        return f"{_linkify(cached)}\n\nâ–²confidence: Cached"

    # 1) ë‚´ë¶€ RAG
    local_ctx, score = _local_ctx(q)
    if local_ctx and score >= THRESH:
        ans, src = await _gate_llm(q, local_ctx)
        if not ans.startswith("ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"):
            asyncio.create_task(_set_cached(_cache_key(q, local_ctx), ans))
            return f"{_linkify(ans)}\n\nâ–²confidence: High ({src})"
        # â€œëª¨ë¥´ê² ìŠµë‹ˆë‹¤â€ ë©´ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰

    # 2) ë£° ê¸°ë°˜ URL
    rule_ctx = ""
    intent = classify_intent_and_extract_entity(q)

    print(">>>>> DEBUG: Intent Result:", intent) 

    if intent.get("intent") == "find_program_url":
        name = intent.get("program_name") or ""
        # --- 1ë‹¨ê³„: 'íƒœê·¸'ì™€ ê±°ì˜ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸ ---
        all_tags = get_all_tags()
        best_tag, s_tag, _ = process.extractOne(name, all_tags, scorer=fuzz.WRatio)
        
        print("teg score : ", s_tag)
        print("best tag: ", best_tag)
        # íƒœê·¸ ì ìˆ˜ê°€ 95ì  ì´ìƒìœ¼ë¡œ ë§¤ìš° ë†’ìœ¼ë©´, ê·¸ë£¹ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼
        if s_tag >= 95:
            programs = get_programs_by_tag(best_tag)
            if programs:
                links = [f"- {p['name']}: {p['url']}" for p in programs]
                ans = f"'{best_tag}' ê´€ë ¨ í˜ì´ì§€ ëª©ë¡ì…ë‹ˆë‹¤.\n\n" + "\n".join(links)
                asyncio.create_task(_set_cached(blank_key, ans))
                return f"{_linkify(ans)}\n\nâ–²confidence: Rule (Group, score={s_tag:.0f})"

        # --- 2ë‹¨ê³„: ì¼ì¹˜í•˜ëŠ” íƒœê·¸ê°€ ì—†ìœ¼ë©´, ê°€ì¥ ë¹„ìŠ·í•œ 'ë³„ì¹­'ì„ ê²€ìƒ‰ ---
        best_alias, s_alias, _ = process.extractOne(name, get_all_aliases(), scorer=fuzz.WRatio)
        
        # ë³„ì¹­ ì ìˆ˜ê°€ 85ì  ì´ìƒì´ë©´ ê°œë³„ í•­ëª©ìœ¼ë¡œ ê°„ì£¼ (ê¸°ì¤€ ì ìˆ˜ ì¡°ì • ê°€ëŠ¥)
        if s_alias >= 85:
            info = get_program_by_alias(best_alias)
            if info:
                ans = f"'{best_alias}' í˜ì´ì§€ì…ë‹ˆë‹¤: {info['url']}"
                asyncio.create_task(_set_cached(blank_key, ans))
                return f"{_linkify(ans)}\n\nâ–²confidence: Rule (Alias, score={s_alias:.0f})"


        # best, s, _ = process.extractOne(name, get_all_aliases(), scorer=fuzz.WRatio)
        # if s >= 75 and (info := get_program_by_alias(best)):
        #     rule_ctx = f"'{best}' í™ˆí˜ì´ì§€: {info['url']}"
        #     ans = rule_ctx
        #     asyncio.create_task(_set_cached(blank_key, ans))
        #     return f"{_linkify(ans)}\n\nâ–²confidence: Rule (score={s:.0f})"

    # 3) í¼ì§€ ë§¤ì¹­
    fuzzy_ctx = _fuzzy_ctx(q)
    if fuzzy_ctx:
        ans, src = await _gate_llm(q, fuzzy_ctx)
        if not ans.startswith("ëª¨ë¥´ê² ìŠµë‹ˆë‹¤") and fact_check(q, ans):
            asyncio.create_task(_set_cached(_cache_key(q, fuzzy_ctx), ans))
            return f"{_linkify(ans)}\n\nâ–²confidence: Mid ({src})"

    # 4) ì›¹ ê²€ìƒ‰
    web_ctx = _web_ctx(q)
    if web_ctx:
        ans, src = await _gate_llm(q, web_ctx)
        if not ans.startswith("ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"):
            asyncio.create_task(_set_cached(_cache_key(q, web_ctx), ans))
            return f"{_linkify(ans)}\n\nâ–²confidence: Mid ({src})"

    # 5) ì¢…í•© ì»¨í…ìŠ¤íŠ¸ ë¬¼ì–´ë³´ê¸° (ìµœì¢… fallback)
    final_ans = _ask_all_sources(q, local_ctx, rule_ctx, web_ctx)
    asyncio.create_task(_set_cached(blank_key, final_ans))
    return f"{_linkify(final_ans)}\n\nâ–²confidence: Low (AllSources)"


'''
async def ask_async(question: str) -> str:
    q = _normalize(question)

    # 0) ìºì‹œ í‚¤(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ ë‹¨ê³„ì—ì„œëŠ” ì§ˆë¬¸ë§Œ ì‚¬ìš©)
    ckey_blank = _cache_key(q, "")
    if (cached := await _get_cached(ckey_blank)):
        return f"{cached}\n\nâ–²confidence: Cached"

    # 1) ë‚´ë¶€ ë¬¸ì„œ RAG â˜…
    ctx, score = _local_ctx(q)
    if ctx and score >= THRESH:
        ans, src = await _gate_llm(q, ctx)
        asyncio.create_task(_set_cached(_cache_key(q, ctx), ans))
        return f"{ans}\n\nâ–²confidence: High ({src})"

    # 2) ë£° ê¸°ë°˜ í”„ë¡œê·¸ë¨ URL â˜…
    intent = classify_intent_and_extract_entity(q, _LLM_BACKUP)
    if intent.get("intent") == "find_program_url":
        name = intent.get("program_name") or ""
        best, s, _ = process.extractOne(name, get_all_aliases(), scorer=fuzz.WRatio)
        if s >= 75 and (info := get_program_by_alias(best)):
            ans = f"'{best}' ë§í¬: [ë°”ë¡œê°€ê¸°]({info['url']})"
            asyncio.create_task(_set_cached(ckey_blank, ans))
            return f"{ans}\n\nâ–²confidence: Rule (score={s:.0f})"

    # 3) í¼ì§€ ë§¤ì¹­
    if (fctx := _fuzzy_ctx(q)):
        ans, src = await _gate_llm(q, fctx)
        if fact_check(q, ans):
            asyncio.create_task(_set_cached(_cache_key(q,fctx), ans))
            return f"{ans}\n\nâ–²confidence: Mid ({src})"

    # 4) ì›¹ ê²€ìƒ‰
    if (wctx := _web_ctx(q)):
        ans, src = await _gate_llm(q, wctx)
        asyncio.create_task(_set_cached(_cache_key(q,wctx), ans))
        return f"{ans}\n\nâ–²confidence: Mid ({src})"

    # 5) ëª¨ë¸ ì˜¨ë¦¬
    ans, src = await _gate_llm(q, "")
    asyncio.create_task(_set_cached(ckey_blank, ans))
    return f"{ans}\n\nâ–²confidence: Low ({src})"

'''