from __future__ import annotations

import asyncio
import contextlib
import hashlib
import html
import json
import os
import re
import textwrap
from dataclasses import dataclass
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from zoneinfo import ZoneInfo
KST = ZoneInfo("Asia/Seoul")
TODAY = datetime.now(KST).date()

# â”€â”€ í”„ë¡œì íŠ¸ ì˜ì¡´ ëª¨ë“ˆ
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from rapidfuzz import fuzz, process
from redis.asyncio import Redis

from app.config import (
    CACHE_TTL,
    DDG_HITS,
    FUZZ_LIMIT,
    FUZZ_SCORE,
    LOCAL_HIT_THRES,
    MAX_COMPLETION_TOKENS,
    OPENAI_API_KEY,
    OPENAI_MODEL,
    OPENAI_TEMPERATURE,
    REDIS_URL,
    validate_runtime_env,
)
from app.rag.intent_classifier import classify_intent_and_entity
from app.rag.prompt import PROMPT_FUSION, PROMPT_SINGLE, STYLE_GUIDE
from app.rag.retriever import get_retriever, get_vectorstore
from app.rag.programs import (
    fuzzy_find_best_alias,
    fuzzy_find_best_tag,
    get_program_by_alias,
    get_programs_by_tag,
)
from app.rag.faq import find_faq_answer

# URL ë¼ìš°í„°(ì‚¬ìš©ìê°€ ë¯¸ë¦¬ ë§¤ì¹­í•´ ë‘” ë§í¬)
try:
    from app.rag.url import find_url_answer
except Exception:
    def find_url_answer(_q: str):
        return None

# ì„¼í„°ì†Œê°œ ì „ìš© ìœ í‹¸
from app.rag.sections.center_intro import build_center_intro_index, query_contact, query_section

# ğŸ”¥ ìƒˆë¡œ ì¶”ê°€ëœ 'ì˜¤ì‹œëŠ” ê¸¸/ì§€ë„' ì „ìš© í›…
from app.rag.hooks.directions import answer_directions

try:
    from app.rag.reranker import rerank  # type: ignore
except Exception:
    def rerank(query: str, docs: List[str], top_k: int = 5):
        return docs[:top_k], 1.0

# í¬ë¡¤ëŸ¬ê°€ ì“°ëŠ” CLEAN ë””ë ‰í† ë¦¬
try:
    from app.config import CLEAN_DIR as _CLEAN_DIR
except Exception:
    _CLEAN_DIR = "app/data/clean"
CLEAN_DIR = Path(_CLEAN_DIR)

# â”€â”€ LLM ë° ê²€ìƒ‰
_SYS = SystemMessage(content="ë„ˆëŠ” ì²œì•ˆì‹œ ë„ì‹œì¬ìƒì§€ì›ì„¼í„° ì „ìš© ì±—ë´‡ì´ë‹¤. ì •í™•í•˜ê³  ê·¼ê±° ìˆëŠ” ì •ë³´ë§Œ ë‹µí•œë‹¤.")
_LLM = ChatOpenAI(
    model=OPENAI_MODEL,
    temperature=OPENAI_TEMPERATURE,
    api_key=OPENAI_API_KEY,
    max_tokens=MAX_COMPLETION_TOKENS,
)
_DDG = DuckDuckGoSearchAPIWrapper()

_redis = Redis.from_url(REDIS_URL, encoding="utf-8", decode_responses=True)
STATE_TTL = int(os.getenv("URC_STATE_TTL", "1800"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¶œë ¥ í¬ë§·í„°(ë§í¬/ì˜¤í† ë§í¬)
_MD_LINK = re.compile(r'\[([^\]]+)\]\((https?://[^\s)]+)\)')
_LABEL_PAREN = re.compile(r'([^\n()]+?)\((https?://[^\s)]+)\)')
_AUTO_URL = re.compile(r'(https?://[^\s<>")]+|www\.[^\s<>")]+)', re.IGNORECASE)
_BAD_LINK_LABEL = re.compile(r"^(ì—¬ê¸°|ë°”ë¡œê°€ê¸°|ë§í¬|í´ë¦­|click|here)$", re.IGNORECASE)

FAQ_STRONG = 90
FAQ_WEAK   = 85

_QUERY_EXPANSIONS = [
    (re.compile(r"ì•„ì¹´ì´ë¸Œ|ìë£Œ\s*ì‹¤", re.IGNORECASE), ["ì•„ì¹´ì´ë¸Œ", "ìë£Œì‹¤", "ì•„ì¹´ì´ë¸Œ í˜ì´ì§€"]),
    (re.compile(r"íˆ¬ì–´|í˜„ì¥\s*íˆ¬ì–´|ì½”ìŠ¤", re.IGNORECASE), ["ë„ì‹œì¬ìƒ íˆ¬ì–´", "í˜„ì¥íˆ¬ì–´", "ì¼ë°˜ì½”ìŠ¤", "ì „ë¬¸ì½”ìŠ¤", "íˆ¬ì–´ ì•ˆë‚´"]),
    (re.compile(r"ì•„ì¹´ë°ë¯¸|êµìœ¡|ê°•ì¢Œ", re.IGNORECASE), ["ë„ì‹œì¬ìƒ ì•„ì¹´ë°ë¯¸", "êµìœ¡", "ê°•ì¢Œ"]),
    (re.compile(r"ì‚¬ì—…ë¹„|ë¹„ìš©|ì˜ˆì‚°", re.IGNORECASE), ["ì‚¬ì—…ë¹„", "ì˜ˆì‚°", "ë¹„ìš©", "ì˜ˆì‚° ì§€ì›"]),
    (re.compile(r"ì‹ ì²­|ëª¨ì§‘", re.IGNORECASE), ["ì‹ ì²­ ë°©ë²•", "ëª¨ì§‘ ì•ˆë‚´", "ì ‘ìˆ˜ ë°©ë²•"]),
    (re.compile(r"ëŒ€ìƒ|ìê²©", re.IGNORECASE), ["ì§€ì› ëŒ€ìƒ", "ì°¸ì—¬ ëŒ€ìƒ", "ìê²© ìš”ê±´"]),
    (re.compile(r"ì¼ì •|ê¸°ê°„|ì‹œê°„", re.IGNORECASE), ["ì¼ì •", "ê¸°ê°„", "ìš´ì˜ ì‹œê°„"]),
    (re.compile(r"ì‚¬ì—…\s*ëª©í‘œ|ì‚¬ì—…ëª©í‘œ|ëª©í‘œ|ì‚¬ì—…\s*ë‚´ìš©|ì‚¬ì—…ë‚´ìš©|ë‚´ìš©|ê°œìš”|ì£¼ìš”\s*ì‚¬ì—…|ì£¼ìš”ì‚¬ì—…|êµ¬ìƒë„", re.IGNORECASE),
     ["ì‚¬ì—…ëª©í‘œ", "ì‚¬ì—…ë‚´ìš©", "ì‚¬ì—…ê°œìš”", "ì£¼ìš”ì‚¬ì—…", "êµ¬ìƒë„"]),
    (re.compile(r"ì¡°ì§ë„|íŒ€ì¥|ë‹´ë‹¹|ì—°ë½|ì—°ë½ì²˜|ì „í™”|ì´ë©”ì¼|ì¹´ì¹´ì˜¤í†¡|ì¹´í†¡|ì˜¤ì‹œëŠ”\s*ê¸¸|ì£¼ì†Œ|ìœ„ì¹˜|ì§€ë„|ì•½ë„", re.IGNORECASE),
     ["ì¡°ì§ë„", "ë‹´ë‹¹ì", "íŒ€ì¥", "ì—°ë½ì²˜", "ì „í™”ë²ˆí˜¸", "ì´ë©”ì¼", "ì˜¤ì‹œëŠ”ê¸¸", "ì£¼ì†Œ", "ìœ„ì¹˜", "ì•½ë„"]),
]

CONTACT_FALLBACK = {
    "phone": ["041-417-4061~5"],
    "email": [],
    "address": ["ì²œì•ˆì‹œ ì€í–‰ê¸¸ 15, 5ì¸µ"],
    "hours": ["í‰ì¼ 09:00â€“18:00"],
    "online_inquiry": "í™ˆí˜ì´ì§€ 'ì˜¨ë¼ì¸ ë¬¸ì˜' ê²Œì‹œíŒì„ ì´ìš©í•´ ì£¼ì„¸ìš”.",
}

_ADDR_LINE = re.compile(r"(ì¶©ë‚¨|ì¶©ì²­ë‚¨ë„|ì²œì•ˆì‹œ)[^\n]{0,80}\d", re.IGNORECASE)
_LATIN_HEAVY = re.compile(r"[A-Za-z]{6,}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìºì‹œ/ìƒíƒœ

def _cache_key(q: str) -> str:
    digest = hashlib.sha256(q.encode("utf-8")).hexdigest()[:24]
    return f"urc_cache:{digest}"


def _state_key(session_id: str) -> str:
    return f"urc_state:{session_id}"


async def _load_state(session_id: Optional[str]) -> Dict:
    if not session_id:
        return {}
    with contextlib.suppress(Exception):
        raw = await _redis.get(_state_key(session_id))
        return json.loads(raw) if raw else {}
    return {}


async def _save_state(session_id: Optional[str], state: Dict):
    if not session_id:
        return
    with contextlib.suppress(Exception):
        await _redis.set(_state_key(session_id), json.dumps(state, ensure_ascii=False), ex=STATE_TTL)


async def _get_cached(key: str) -> Optional[str]:
    with contextlib.suppress(Exception):
        return await _redis.get(key)
    return None


async def _set_cached(key: str, val: str, ttl: int = CACHE_TTL):
    with contextlib.suppress(Exception):
        await _redis.set(key, val, ex=ttl)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¶œë ¥ í¬ë§·

def _anchor(url: str, label: Optional[str] = None) -> str:
    u = url if url.startswith(("http://", "https://")) else f"https://{url}"
    lab = label or u
    return f'<a href="{u}" target="_blank" rel="noopener noreferrer">{html.escape(lab)}</a>'


def _to_html(text: str) -> str:
    if not text:
        return ""
    s = html.unescape(str(text))

    def _mk(m):
        label, url = m.group(1).strip(), m.group(2).strip()
        if re.match(r"^(ì—¬ê¸°|ë°”ë¡œê°€ê¸°|ë§í¬|í´ë¦­|click|here)$", label, re.IGNORECASE) or len(label) <= 4:
            return _anchor(url, url)
        return _anchor(url, label)

    s = _MD_LINK.sub(_mk, s)
    s = _LABEL_PAREN.sub(_mk, s)

    parts = re.split(r'(<[^>]+>)', s)
    for i, part in enumerate(parts):
        if not part or part.startswith("<"):
            continue
        parts[i] = _AUTO_URL.sub(lambda m: _anchor(m.group(0), m.group(0)), part)
    s = "".join(parts)
    return s.replace("\n", "<br>")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œì»¬ RAG/ì›¹ ë³´ê°•(ê¸°ì¡´)

def _shorten(texts: List[str], width: int = 420) -> List[str]:
    return [textwrap.shorten(t, width, placeholder="â€¦") for t in texts if t and t.strip()]


def _expand_queries(q: str) -> List[str]:
    out = {q}
    for pat, repls in _QUERY_EXPANSIONS:
        if pat.search(q):
            out.update(repls)
    q2 = re.sub(r"(ì´ë€?|ì´ì•¼|ë­[ì•¼ìš”]?|ê°€?\s*ê¶ê¸ˆ|ì•Œë ¤ì¤˜|ë³´ì—¬ì¤˜|ì–´ë””ì„œë´\??|ì–´ë””ì„œ\s*ë´\??|ì–´ë””ì„œ\s*í™•ì¸\??)", "", q).strip()
    if q2 and q2 != q:
        out.add(q2)
    return list(out)


def _local_ctx(q: str) -> Tuple[str, float, int]:
    queries = [q] + [x for x in _expand_queries(q) if x != q]
    seen = set()
    docs_all: List = []
    retriever = get_retriever()

    for qv in queries[:6]:
        try:
            docs = retriever.get_relevant_documents(qv)
        except Exception:
            docs = []
        for d in docs:
            key = (d.page_content, tuple(sorted((d.metadata or {}).items())))
            if key in seen:
                continue
            seen.add(key)
            docs_all.append(d)

    nraw = len(docs_all)
    if not docs_all:
        return "", 0.0, 0

    contents = [d.page_content for d in docs_all]
    try:
        top_strings, best = rerank(q, contents)
        used = []
        i = 0
        for s in top_strings:
            while i < len(docs_all):
                if docs_all[i].page_content == s:
                    used.append(docs_all[i])
                    i += 1
                    break
                i += 1
        if not used:
            used = docs_all[:6]
        best_score = float(best)
    except Exception:
        used = docs_all[:6]
        best_score = 1.0

    blocks = []
    for d in used:
        meta = d.metadata or {}
        title = meta.get("title") or ""
        section = meta.get("section") or ""
        url = meta.get("url") or meta.get("source") or ""
        head = f"[{title}{(' > ' + section) if section else ''}]".strip()
        tail = f"\nì¶œì²˜: {url}" if url else ""
        blocks.append(f"{head}\n{d.page_content}{tail}")

    ctx = "\n\n---\n\n".join(blocks)
    return ctx, best_score, nraw


def _fuzzy_ctx(q: str) -> Optional[str]:
    vs = get_vectorstore()
    texts = [d.page_content for d in getattr(vs.docstore, "_dict", {}).values()]
    if not texts:
        return None
    pairs = process.extract(q, texts, scorer=fuzz.partial_ratio, limit=FUZZ_LIMIT)
    chosen = [t for t, score, _ in pairs if score >= FUZZ_SCORE]
    if not chosen:
        return None
    return "\n\n".join(_shorten(chosen))


def _format_hits(hits: List[dict], max_items: int) -> Optional[str]:
    if not hits:
        return None
    out = []
    for h in hits[:max_items]:
        title = h.get("title") or h.get("snippet") or h.get("link")
        link = h.get("link")
        if not link:
            continue
        out.append(_anchor(link, title))
    return "<br>".join(out) if out else None


def _web_ctx(q: str) -> Optional[str]:
    with contextlib.suppress(Exception):
        hits = _DDG.results(q, max_results=DDG_HITS)
        return _format_hits(hits, DDG_HITS)
    return None


def _web_fallback_answer(q: str) -> Optional[str]:
    with contextlib.suppress(Exception):
        hits = _DDG.results(q, max_results=5)
        if not hits:
            return None
        lines = []
        for h in hits[:5]:
            title = h.get("title") or h.get("link")
            link = h.get("link")
            if not link:
                continue
            lines.append(f"- {_anchor(link, title)}")
        return "ë‚´ ë¬¸ì„œì—ì„œ ì •í™•íˆ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ìŒ ìë£Œë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”:\n\n" + "\n".join(lines)
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡œê·¸ë¨ ê¸°ê°„/ìƒíƒœ ì§ˆì˜ ì²˜ë¦¬ (ê³¼ê±° í¬í•¨)
@dataclass
class ProgramDoc:
    title: str
    url: str
    text_path: str
    status: Optional[str]  # ì˜ˆì •/ì§„í–‰ì¤‘/ë§ˆê°
    start_date: Optional[date]
    end_date: Optional[date]

    def period_str(self) -> str:
        if self.start_date and self.end_date:
            return f"{self.start_date:%Y-%m-%d} ~ {self.end_date:%Y-%m-%d}"
        if self.start_date:
            return f"{self.start_date:%Y-%m-%d} ~"
        if self.end_date:
            return f"~ {self.end_date:%Y-%m-%d}"
        return "ê¸°ê°„ ì •ë³´ ì—†ìŒ"


def _parse_date(s: Optional[str]) -> Optional[date]:
    if not s:
        return None
    try:
        return datetime.strptime(s, "%Y-%m-%d").date()
    except Exception:
        return None


def load_all_manifests() -> List[ProgramDoc]:
    out: List[ProgramDoc] = []
    if not CLEAN_DIR.exists():
        return out
    for mf in CLEAN_DIR.glob("**/manifest.jsonl"):
        with contextlib.suppress(Exception):
            for ln in mf.read_text(encoding="utf-8").splitlines():
                if not ln.strip():
                    continue
                rec = json.loads(ln)
                out.append(
                    ProgramDoc(
                        title=rec.get("title") or "",
                        url=rec.get("url") or "",
                        text_path=rec.get("text_path") or "",
                        status=(rec.get("status") or None),
                        start_date=_parse_date(rec.get("start_date")),
                        end_date=_parse_date(rec.get("end_date")),
                    )
                )
    return out


ABS_RANGE = re.compile(
    r"(?P<y1>20\d{2})[.\-ë…„/ ]*(?P<m1>\d{1,2})?[.\-ì›”/ ]*(?P<d1>\d{1,2})?" r"\s*[~\-â€“]\s*"
    r"(?P<y2>20\d{2})[.\-ë…„/ ]*(?P<m2>\d{1,2})?[.\-ì›”/ ]*(?P<d2>\d{1,2})?"
)
ABS_ONE = re.compile(r"(?P<y>20\d{2})[.\-ë…„/ ]*(?P<m>\d{1,2})?[.\-ì›”/ ]*(?P<d>\d{1,2})?")
REL_WORDS = {"ì‘ë…„": -1, "ì§€ë‚œí•´": -1, "ì˜¬í•´": 0, "ê¸ˆë…„": 0, "ë‚´ë…„": 1, "ë‹¤ìŒí•´": 1}
STATUS_WORDS = {"ì§„í–‰": "ì§„í–‰ì¤‘", "ì§„í–‰ì¤‘": "ì§„í–‰ì¤‘", "ëª¨ì§‘ì¤‘": "ì§„í–‰ì¤‘", "ì˜ˆì •": "ì˜ˆì •", "ë§ˆê°": "ë§ˆê°", "ì¢…ë£Œ": "ë§ˆê°"}
Q_WORDS = ("í”„ë¡œê·¸ë¨", "ëª¨ì§‘", "ì‹ ì²­", "ì ‘ìˆ˜", "êµìœ¡", "ê³µëª¨", "í–‰ì‚¬")


def is_program_date_query(q: str) -> bool:
    has_kw = any(k in q for k in Q_WORDS)
    has_time = bool(
        ABS_RANGE.search(q)
        or ABS_ONE.search(q)
        or any(w in q for w in REL_WORDS.keys())
        or any(
            w in q
            for w in [
                "ì§€ë‚œë‹¬",
                "ì´ë²ˆë‹¬",
                "ë‹¤ìŒë‹¬",
                "ì¬ì‘ë…„",
                "ìƒë°˜ê¸°",
                "í•˜ë°˜ê¸°",
                "1ë¶„ê¸°",
                "2ë¶„ê¸°",
                "3ë¶„ê¸°",
                "4ë¶„ê¸°",
                "ê¸°ê°„",
            ]
        )
    )
    return has_kw and has_time


def month_start(dt: date) -> date:
    return date(dt.year, dt.month, 1)


def month_end(dt: date) -> date:
    if dt.month == 12:
        return date(dt.year, 12, 31)
    first_next = date(dt.year, dt.month + 1, 1)
    return first_next - timedelta(days=1)


def quarter_bounds(year: int, q: int) -> Tuple[date, date]:
    m1 = (q - 1) * 3 + 1
    start = date(year, m1, 1)
    end = month_end(date(year, m1 + 2, 1))
    return start, end


def half_bounds(year: int, half: str) -> Tuple[date, date]:
    if half == "ìƒë°˜ê¸°":
        return date(year, 1, 1), date(year, 6, 30)
    return date(year, 7, 1), date(year, 12, 31)


def parse_korean_date_range(q: str, today: date = TODAY) -> Tuple[Optional[date], Optional[date]]:
    # ë²”ìœ„
    m = ABS_RANGE.search(q)
    if m:
        y1, m1, d1 = int(m.group("y1")), m.group("m1"), m.group("d1")
        y2, m2, d2 = int(m.group("y2")), m.group("m2"), m.group("d2")
        m1i = int(m1) if m1 else 1
        d1i = int(d1) if d1 else 1
        m2i = int(m2) if m2 else 12
        d2i = int(d2) if d2 else month_end(date(y2, m2i, 1)).day
        return date(y1, m1i, d1i), date(y2, m2i, d2i)

    # ë‹¨ì¼
    m = ABS_ONE.search(q)
    if m:
        y = int(m.group("y"))
        if m.group("m"):
            m_i = int(m.group("m"))
            if m.group("d"):
                d_i = int(m.group("d"))
                return date(y, m_i, d_i), date(y, m_i, d_i)
            return date(y, m_i, 1), month_end(date(y, m_i, 1))
        return date(y, 1, 1), date(y, 12, 31)

    # ìƒëŒ€
    for w, delta in REL_WORDS.items():
        if w in q:
            y = today.year + delta
            return date(y, 1, 1), date(y, 12, 31)

    # ì§€ë‚œë‹¬/ì´ë²ˆë‹¬/ë‹¤ìŒë‹¬
    if "ì§€ë‚œë‹¬" in q:
        y, m = today.year, today.month
        if m == 1:
            y -= 1
            m = 12
        else:
            m -= 1
        return date(y, m, 1), month_end(date(y, m, 1))
    if "ì´ë²ˆë‹¬" in q or "ì´ë‹¬" in q or "ì´ë²ˆ ë‹¬" in q:
        return month_start(today), month_end(today)
    if "ë‹¤ìŒë‹¬" in q:
        y, m = today.year, today.month
        if m == 12:
            y += 1
            m = 1
        else:
            m += 1
        return date(y, m, 1), month_end(date(y, m, 1))

    # ë¶„ê¸°/ë°˜ê¸°
    for qi in (1, 2, 3, 4):
        if f"{qi}ë¶„ê¸°" in q:
            return quarter_bounds(today.year, qi)
    if "ìƒë°˜ê¸°" in q:
        return half_bounds(today.year, "ìƒë°˜ê¸°")
    if "í•˜ë°˜ê¸°" in q:
        return half_bounds(today.year, "í•˜ë°˜ê¸°")

    return None, None


def detect_status_filter(q: str) -> Optional[str]:
    for k, v in STATUS_WORDS.items():
        if k in q:
            return v
    return None


def overlaps(a_start: Optional[date], a_end: Optional[date], b_start: Optional[date], b_end: Optional[date]) -> bool:
    a_s = a_start or date.min
    a_e = a_end or date.max
    b_s = b_start or date.min
    b_e = b_end or date.max
    return not (a_e < b_s or b_e < a_s)


def filter_programs(
    docs: List[ProgramDoc],
    req_start: Optional[date],
    req_end: Optional[date],
    status_filter: Optional[str],
) -> List[ProgramDoc]:
    out = []
    for d in docs:
        if status_filter and (d.status or "").strip() != status_filter:
            continue
        if req_start or req_end:
            if not overlaps(d.start_date, d.end_date, req_start, req_end):
                continue
        out.append(d)

    def sort_key(x: ProgramDoc):
        sd = x.start_date or date.min
        return (sd, x.title)

    return sorted(out, key=sort_key, reverse=True)


def format_program_list_answer(
    filtered: List[ProgramDoc],
    req_start: Optional[date],
    req_end: Optional[date],
    status_filter: Optional[str],
    limit: int = 20,
) -> str:
    hdr = "ìš”ì²­í•˜ì‹ "
    if req_start and req_end:
        hdr += f" ê¸°ê°„({req_start:%Y-%m-%d} ~ {req_end:%Y-%m-%d})"
    elif req_start:
        hdr += f" {req_start:%Y-%m-%d} ì´í›„"
    elif req_end:
        hdr += f" {req_end:%Y-%m-%d} ì´ì „"
    else:
        hdr += " ê¸°ê°„"

    if status_filter:
        hdr += f"ì˜ **{status_filter}** ìƒíƒœ í”„ë¡œê·¸ë¨ ëª©ë¡ì…ë‹ˆë‹¤.\n\n"
    else:
        hdr += "ì˜ í”„ë¡œê·¸ë¨ ëª©ë¡ì…ë‹ˆë‹¤.\n\n"

    if not filtered:
        none_msg = f"{hdr}í•´ë‹¹ë˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        if status_filter is None:
            return none_msg + " (ê²€ìƒ‰ íŒ: '2024ë…„ 5ì›” í”„ë¡œê·¸ë¨', '2023ë…„ í•˜ë°˜ê¸° ë§ˆê° í”„ë¡œê·¸ë¨'ì²˜ëŸ¼ ê¸°ê°„/ìƒíƒœë¥¼ í•¨ê»˜ ì ì–´ë³´ì„¸ìš”.)"
        return none_msg

    lines = [hdr]
    for i, d in enumerate(filtered[:limit], start=1):
        st = d.status or "ìƒíƒœ ì •ë³´ ì—†ìŒ"
        lines.append(f"{i}. [{d.title}]({d.url})  \n   ê¸°ê°„: {d.period_str()}  \n   ì§„í–‰ìƒíƒœ: **{st}**")
    if len(filtered) > limit:
        lines.append(f"\nâ€¦ ì™¸ {len(filtered) - limit}ê±´")
    return "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¼í„°ì†Œê°œ ì „ìš© í›… (ì£¼ì†Œ/ì§€ë„ ì œì™¸)
_CI_HINT = re.compile(r"(ì„¼í„°\s*ì†Œê°œ|ì¸ì‚¬ë§|ì—°í˜|ì¡°ì§ë„|ëª©í‘œ|ë¹„ì „)", re.IGNORECASE)


def _answer_center_intro(q: str) -> Optional[str]:
    """'ì„¼í„°ì†Œê°œ' ê´€ë ¨ ê°„ë‹¨ ì§ˆì˜ì— ë°”ë¡œ ì‘ë‹µ(ì¸ì‚¬ë§/ì—°í˜/ì¡°ì§ë„/ëª©í‘œÂ·ë¹„ì „).\n    â€» ì£¼ì†Œ/ì§€ë„ëŠ” hooks.directions ê°€ ì „ë‹´"""
    if not _CI_HINT.search(q):
        return None

    idx = build_center_intro_index()

    if re.search(r"ì¸ì‚¬ë§", q):
        blocks = query_section(idx, "ì¸ì‚¬ë§")
        if blocks:
            return _to_html("\n\n".join(blocks))
    if re.search(r"ì—°í˜", q):
        blocks = query_section(idx, "ì—°í˜")
        if blocks:
            return _to_html("\n\n".join(blocks))
    if re.search(r"ì¡°ì§ë„", q):
        blocks = query_section(idx, "ì¡°ì§ë„")
        if blocks:
            return _to_html("\n\n".join(blocks))
    if re.search(r"(ëª©í‘œ|ë¹„ì „)", q):
        blocks = query_section(idx, "ëª©í‘œë¹„ì „")
        if blocks:
            return _to_html("\n\n".join(blocks))

    # 'ì„¼í„° ì†Œê°œ'ë§Œ ë¬¼ì—ˆì„ ë•Œ: ì¸ì‚¬ë§ ì¼ë¶€ + ì—°ë½ì²˜ ìš”ì•½
    blocks = query_section(idx, "ì¸ì‚¬ë§")
    contacts = query_contact(idx)
    if blocks or contacts:
        msg = []
        if blocks:
            msg.append("\n\n".join(blocks[:1]))
        if contacts:
            msg.append("### ì—°ë½ì²˜ ìš”ì•½\n" + "\n".join(contacts))
        return _to_html("\n\n".join(msg))

    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì§„ì…ì 
async def ask_async(question: str, session_id: Optional[str] = None) -> str:
    with contextlib.suppress(Exception):
        validate_runtime_env()

    q = (question or "").strip()
    if not q:
        return _to_html("ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

    state = await _load_state(session_id)
    cache_key = _cache_key((session_id or "") + "|" + q)
    if cached := await _get_cached(cache_key):
        return _to_html(cached)

    # 0) âœ… URL ë¼ìš°í„°ê°€ ìµœìš°ì„ 
    hit = find_url_answer(q)
    if hit:
        html_out = hit.html if hasattr(hit, "html") else str(hit)
        asyncio.create_task(_set_cached(cache_key, html_out))
        await _save_state(session_id, {**state, "last_intent": "url_router"})
        return _to_html(html_out)

    # 1) ê·¸ë‹¤ìŒ: ì˜¤ì‹œëŠ” ê¸¸/ì§€ë„ (ì£¼ì†Œ í‚¤ì›Œë“œ ì œê±°ë˜ì–´ ê³¼ë°œë™ ë°©ì§€)
    ans_dir = answer_directions(q)
    if ans_dir:
        asyncio.create_task(_set_cached(cache_key, ans_dir))
        await _save_state(session_id, {**state, "last_intent": "directions"})
        return _to_html(ans_dir)

    # 2) FAQ ì´ˆê°•ë§¤ì¹­
    faq_exact = find_faq_answer(q, hard_threshold=100, soft_threshold=100)
    if faq_exact:
        asyncio.create_task(_set_cached(cache_key, faq_exact))
        await _save_state(session_id, {**state, "last_intent": "faq"})
        return _to_html(faq_exact)

    # 3) ì„¼í„°ì†Œê°œ(ì£¼ì†Œ/ì§€ë„ ì œì™¸)
    ci = _answer_center_intro(q)
    if ci:
        asyncio.create_task(_set_cached(cache_key, ci))
        await _save_state(session_id, {**state, "last_intent": "center_intro"})
        return ci

    # 4) ì—°ë½ì²˜ ì˜ë„ì¼ ë•Œë„ url.py â†’ directions ìˆœìœ¼ë¡œ ì¬í™•ì¸
    info = classify_intent_and_entity(q) or {}
    if info.get("intent") == "ask_contact":
        ctype = info.get("contact_type") or ""
        if ctype in {"address", "location", "map"}:
            hit2 = find_url_answer(q)
            if hit2:
                html_out = hit2.html if hasattr(hit2, "html") else str(hit2)
                asyncio.create_task(_set_cached(cache_key, html_out))
                await _save_state(session_id, {**state, "last_intent": "url_router"})
                return _to_html(html_out)
            ans = answer_directions(q)
            if ans:
                asyncio.create_task(_set_cached(cache_key, ans))
                await _save_state(session_id, {**state, "last_intent": "directions"})
                return _to_html(ans)

    # 5) í”„ë¡œê·¸ë¨ ê¸°ê°„/ìƒíƒœ
    q_norm = q.lower()
    if is_program_date_query(q_norm):
        docs = await asyncio.to_thread(load_all_manifests)
        req_start, req_end = parse_korean_date_range(q_norm)
        status_filter = detect_status_filter(q_norm)
        filtered = filter_programs(docs, req_start, req_end, status_filter)
        answer = format_program_list_answer(filtered, req_start, req_end, status_filter)
        asyncio.create_task(_set_cached(cache_key, answer))
        await _save_state(session_id, {**state, "last_intent": "program_period"})
        return _to_html(answer)

    # 6) ë¡œì»¬ â†’ LLM
    local_ctx, best, nraw = _local_ctx(q)
    if local_ctx and (best >= LOCAL_HIT_THRES or nraw > 0):
        ans_local = _llm_single(q, local_ctx)
        if ans_local and not re.match(r"^(ëª¨ë¥´ê² |ì˜ ì•Œ ìˆ˜ ì—†|í™•ì¸ì´ í•„ìš”|ì •ë³´ê°€ ë¶€ì¡±)", ans_local):
            asyncio.create_task(_set_cached(cache_key, ans_local))
            await _save_state(session_id, {**state, "last_intent": "ask_info"})
            return _to_html(ans_local)

    # 7) FAQ(ì•½)
    faq_ans_soft = find_faq_answer(q, hard_threshold=FAQ_WEAK, soft_threshold=FAQ_WEAK)
    if faq_ans_soft:
        asyncio.create_task(_set_cached(cache_key, faq_ans_soft))
        await _save_state(session_id, {**state, "last_intent": "faq"})
        return _to_html(faq_ans_soft)

    # 8) í¼ì§€ + LLM
    fuzzy_ctx = _fuzzy_ctx(q)
    if fuzzy_ctx:
        ans_fuzzy = _llm_single(q, fuzzy_ctx)
        if ans_fuzzy and not re.match(r"^(ëª¨ë¥´ê² |ì˜ ì•Œ ìˆ˜ ì—†|í™•ì¸ì´ í•„ìš”|ì •ë³´ê°€ ë¶€ì¡±)", ans_fuzzy):
            asyncio.create_task(_set_cached(cache_key, ans_fuzzy))
            await _save_state(session_id, {**state, "last_intent": "ask_info"})
            return _to_html(ans_fuzzy)

    # 9) ì›¹ í´ë°±
    web_summary = _web_fallback_answer(q)
    if web_summary:
        asyncio.create_task(_set_cached(cache_key, web_summary))
        await _save_state(session_id, {**state, "last_intent": "web_fallback"})
        return _to_html(web_summary)

    # 10) ìµœì¢… ìœµí•©
    web_ctx = _web_ctx(q) or ""
    final = _llm_fusion(q, local_ctx or fuzzy_ctx or "", "", web_ctx)
    asyncio.create_task(_set_cached(cache_key, final))
    await _save_state(session_id, {**state, "last_intent": "ask_info"})
    return _to_html(final)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM í˜¸ì¶œ ë˜í¼

def _llm_single(q: str, ctx: str) -> str:
    msg = PROMPT_SINGLE.format(style=STYLE_GUIDE, context=ctx or "ì—†ìŒ", question=q)
    return _LLM.invoke([_SYS, HumanMessage(content=msg)]).content.strip()


def _llm_fusion(q: str, local_ctx: str, rule_ctx: str, web_ctx: str) -> str:
    msg = PROMPT_FUSION.format(
        style=STYLE_GUIDE,
        local_ctx=local_ctx or "ì—†ìŒ",
        rule_ctx=rule_ctx or "ì—†ìŒ",
        web_ctx=web_ctx or "ì—†ìŒ",
        question=q,
    )
    return _LLM.invoke([_SYS, HumanMessage(content=msg)]).content.strip()